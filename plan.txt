### Project Plan: Real-Time Speech Translation in Online Conferences

**Overall Goal:** Develop a real-time speech translation system with voice cloning, achieving under 10 seconds latency, supporting multiple languages (EN↔SK initially), with a backend-focused development and a simple frontend for testing.

**Phase 1: Environment Setup & Dependency Management**
1.  **Conda Environment Creation:** Create a dedicated Conda environment named `BP` to manage all project dependencies.
2.  **Dependency Research & `requirements.txt`:**
    *   Identify specific versions of `chatterbox-tts`, `piper-tts`, `transformers`, `torch` (with MPS support), `sentencepiece` (for NLLB), and `mlx-whisper` (or `faster-whisper` if MPS issue is resolved) that are compatible and optimized for Mac M1 Pro (Apple Silicon MPS acceleration).
    *   Create a comprehensive `requirements.txt` file to ensure all dependencies are listed and version-locked, preventing mismatches.
    *   Investigate any potential conflicts between `chatterbox-tts` (PyTorch/MPS), `piper-tts` (ONNX/C++), and `mlx-whisper` (MLX framework) and ensure a unified installation strategy.

**Phase 2: Backend Development - Core Modules**
1.  **Speech-to-Text (STT) Module (`mlx-whisper`):**
    *   Implement real-time audio streaming and processing.
    *   `mlx-whisper` is integrated and working for transcription on Mac M1 Pro.
    *   Incorporate Voice Activity Detection (VAD) for efficient processing of conference audio.
    *   Handle multilingual input for STT.
2.  **Machine Translation (MT) Module (Re-evaluation needed):**
    *   `NLLB-200-distilled` is currently producing garbage output on MPS, indicating compatibility issues.
    *   **Action:** Research and integrate an alternative MT model known for better MPS compatibility, or implement a robust CPU-only fallback for NLLB if no suitable MPS-accelerated alternative is found. Prioritize models that can be quantized for lower latency.
    *   Configure for EN ↔ SK translation, with extensibility for other languages.
3.  **Text-to-Speech (TTS) Module (`chatterbox` & `piper`):**
    *   Implement a dual-model TTS system:
        *   **Chatterbox:** Currently facing `get_speaker_embedding` API issues and `torch` version conflicts, temporarily disabled. Further investigation or alternative required for voice cloning.
        *   **Piper:** Integrated, but requires manual download of model files for full functionality.
    *   Develop logic for dynamic model selection based on latency or quality requirements.
    *   Integrate voice cloning using a short reference audio (e.g., `test/Voice-Training.wav`).
4.  **Voice Cloning Integration:**
    *   Currently blocked by Chatterbox `get_speaker_embedding` issues. Will revisit once Chatterbox is stable or an alternative is chosen.
    *   Pass these embeddings to the TTS model (Chatterbox) for personalized translated speech.
5.  **Backend API & WebSocket:**
    *   Set up a Python backend using FastAPI and WebSockets for real-time communication with the frontend.
    *   Define API endpoints for initialization, configuration updates, and streaming audio data.

**Phase 3: Backend Testing Workflow**
1.  **Module Initialization:** Implement a function to initialize all STT, MT, and TTS models, loading them with appropriate quantization and MPS acceleration.
2.  **Voice Training:** Develop a function to train the voice cloning model using `test/Voice-Training.wav`. This will involve extracting and storing speaker embeddings.
3.  **Input Processing:** Implement a workflow to take `test/My test speech_xtts_speaker_clean.wav` as input, pass it through the STT, MT, and TTS pipeline.
4.  **Output Generation:** Generate the translated and synthesized speech output.
5.  **Latency Measurement:** Integrate robust timing mechanisms to measure STT, MT, TTS, and total end-to-end latency, ensuring the <10s target is met.
6.  **Multilingual Testing:** Test the workflow for EN→SK and SK→EN translation.

**Phase 4: Frontend Integration (for testing)**
1.  **UI Connection:** Connect the existing `ui/index.html`, `ui/script.js`, and `ui/style.css` to the backend via WebSockets.
2.  **Real-time Display:** Update the UI to display transcription, translation, and latency metrics in real-time.
3.  **Audio I/O:** Implement microphone input and synthesized audio output through the frontend.
4.  **Voice Training Modal:** Ensure the voice training modal correctly interacts with the backend for prompt generation and reference audio upload/recording.

**Phase 5: Production Version Considerations**
1.  **Audio Input:** Plan for switching to Blackhole 2ch input for capturing conference platform audio streams.
2.  **Audio Output:** Consider options for virtual speakers or direct audio injection into conference platforms.
