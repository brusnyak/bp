# Concurrent Performance Test Results

## Executive Summary

This document presents the results of concurrent performance testing for the Live Speech Translation system. The test simulated multiple simultaneous users to validate system capacity and measure scalability.

**Test Date:** November 28, 2025, 11:30  
**Test Environment:** M1 Pro 16GB, macOS  
**TTS Model:** Piper (lightweight)  
**Configuration:** 15.0s random ramp-up delay, increased WebSocket concurrency limit

### Key Findings

‚úÖ **Verified Maximum Capacity: 12 Concurrent Users (100% Success)**  
‚ö†Ô∏è **Capacity Limit Reached:** 15 Users (73% Success)  
üìä **Average E2E Latency:** ~29s at capacity (due to queuing)  
üöÄ **Bottleneck:** Memory/CPU saturation from per-user model instances

---

## Test Configuration

### Test Parameters

- **Concurrent Users Tested:** 12, 15
- **Audio Files Used:**
  - `Hello.wav` - Short greeting (24 words)
  - `Can you hear me_.wav` - Quick question (7 words)
  - `My test speech_xtts_speaker_clean.wav` - Long test (65 words)
- **Languages:** English ‚Üí Slovak
- **TTS Engine:** Piper (cs_CZ-jirka-medium model)
- **Ramp-up Strategy:** Random start times over 15.0 seconds

### Test Methodology

1. Users connect with random delays within 15s window
2. Each user sends their audio file
3. System processes: STT ‚Üí MT ‚Üí TTS
4. Timeline events recorded with microsecond precision

---

## Test Results

### 12 Concurrent Users (Capacity Limit)

**‚úÖ Success Rate: 100% (12/12 sessions completed)**

| Metric      | Average | Min    | Max    | Std Dev |
| ----------- | ------- | ------ | ------ | ------- |
| E2E Latency | 29.22s  | 26.62s | 31.20s | 1.92s   |
| STT Time    | 3.58s   | 0.65s  | 9.93s  | 2.70s   |
| MT Time     | 1.36s   | 0.16s  | 4.39s  | 1.28s   |
| TTS Time    | 1.03s   | 0.35s  | 1.96s  | 0.62s   |

![Timeline Gantt Chart - 12 Users](file:///Users/yegor/Documents/STU/BP/test_output/performance_tests/run_20251128_111937/timeline_gantt_12_users.png)

**Analysis:**

- System successfully handled 12 simultaneous users
- Latency increased significantly (avg 29s) compared to 5 users (avg 10s), indicating heavy queuing
- STT processing time increased to ~3.6s avg, confirming CPU saturation

![Latency Distributions - 12 Users](file:///Users/yegor/Documents/STU/BP/test_output/performance_tests/run_20251128_111937/latency_distributions_12_users.png)

---

### 15 Concurrent Users (Overload)

**‚ö†Ô∏è Success Rate: 73.3% (11/15 sessions completed)**

| Metric      | Average | Min    | Max    | Std Dev |
| ----------- | ------- | ------ | ------ | ------- |
| E2E Latency | 25.08s  | 24.39s | 26.21s | 0.80s   |
| STT Time    | 5.69s   | 3.64s  | 6.98s  | 1.47s   |
| MT Time     | 1.82s   | 1.20s  | 2.38s  | 0.48s   |
| TTS Time    | 1.41s   | 1.38s  | 1.44s  | 0.02s   |

![Timeline Gantt Chart - 15 Users](file:///Users/yegor/Documents/STU/BP/test_output/performance_tests/run_20251128_111937/timeline_gantt_15_users.png)

**Analysis:**

- 4 sessions failed (likely connection timeouts or server-side OOM/crash for those threads)
- Successful sessions had high latency
- STT time spiked to ~5.7s avg
- **Conclusion:** 15 users exceeds the stable capacity of the M1 Pro with the current architecture.

---

## Detailed Performance Analysis

### Scalability Verification

1. **Stable Zone (1-10 Users):** System performs with low latency (<10s) and 100% reliability.
2. **Saturation Zone (10-12 Users):** System remains reliable (100% success) but latency increases significantly due to queuing.
3. **Failure Zone (>12 Users):** System begins to drop connections or fail requests due to resource exhaustion.

### Bottleneck Identification

The primary bottleneck is **Memory and CPU saturation** caused by the **per-user model instantiation** architecture.

- Each user session creates new instances of STT, MT, and TTS models.
- At 12 users, the 16GB Unified Memory is likely fully utilized, causing swapping and slowdowns.
- At 15 users, the system runs out of resources, leading to failures.

---

## Recommendations

### Configuration for Production

1. **Uvicorn Settings:**

   ```python
   uvicorn.run(app, limit_concurrency=100, backlog=1024)
   ```

2. **Capacity Planning:**
   - **Safe Capacity:** 10 concurrent users on M1 Pro.
   - **Max Capacity:** 12 concurrent users.
   - **Hardware Upgrade:** Moving to a server with 64GB+ RAM and CUDA GPU would significantly increase capacity (estimated 50+ users).

### Architectural Improvements

To scale beyond 12 users on limited hardware:

1. **Model Sharing:** Refactor backend to use a shared pool of model instances (e.g., 2-4 worker threads) rather than 1 instance per user.
2. **Request Queuing:** Implement a global queue for inference requests to prevent resource exhaustion.

---

## Test Artifacts

All test data is available in: `test_output/performance_tests/run_20251128_111937/`
